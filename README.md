# ShopTalk Search Assistant

An intelligent e-commerce product search system that uses natural language processing, RAG (Retrieval-Augmented Generation), and LLM-powered natural language generation to help users find products through conversational queries.

## ğŸ—ï¸ High-Level Architecture

```mermaid
graph TB
    subgraph "Data Pipeline"
        A[ABO Dataset] --> B[Research Notebook]
        B --> C[Embedding Models Comparison]
        C --> D[Best Model: e5-base]
        D --> E[Fine-tuning with Triplet Loss]
        E --> F[Embeddings â†’ Parquet]
        F --> G[Git LFS Storage]
    end
    
    subgraph "Ingestion Pipeline"
        G --> H[Ingest Script]
        H --> I[Chroma Vector DB]
    end
    
    subgraph "Search Pipeline"
        J[User Query] --> K[Streamlit UI]
        K --> L[LLM Helper - Query Parsing]
        L --> M[Vector Search]
        M --> N[Cross-Encoder Reranking]
        N --> O[LLM Helper - Response Generation]
        O --> P[Natural Language Response]
    end
    
    I --> M
```

## ğŸ”§ System Components

### 1. **Research & Model Selection** (`research/`)
- **Notebook**: `research/eda/ShopTalk_ABO_EDA_Embeddings.ipynb`
- **Purpose**: Download ABO dataset, compare 3 pre-trained embedding models
- **Metrics**: Recall@10 and NDCG@10
- **Winner**: e5-base model
- **Fine-tuning**: Triplet loss optimization for better product embeddings

### 2. **Data Storage** (`data/`)
- **Parquet Files**: Product embeddings stored as compressed parquet
- **Git LFS**: Large files tracked with Git Large File Storage
- **Manifest**: Model metadata and training information

### 3. **Ingestion Pipeline** (`ingest/`)
- **Scripts**: Load and rebuild local Chroma vector database
- **Vector DB**: Persistent storage for semantic search
- **Indexing**: HNSW-based cosine similarity search

### 4. **API Layer** (`api/`)
- **FastAPI**: RESTful API for search endpoints
- **LLM Helper**: LangChain integration with OpenAI for query parsing and response generation
- **Vector Search**: Semantic similarity search with filtering
- **Reranking**: Cross-encoder model for result refinement

### 5. **User Interface** (`ui/`)
- **Streamlit**: Interactive web interface for natural language queries
- **Real-time Search**: Live search results with product details
- **Responsive Design**: User-friendly product discovery experience

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Git LFS (for large file handling)
- OpenAI API key

### 1. Clone and Setup
```bash
# Clone the repository
git clone <repository-url>
cd shoptalk-search-assistant

# Install Git LFS (if not already installed)
git lfs install
git lfs pull

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies
```bash
# Install API dependencies
pip install -r api/requirements.txt

# Install ingestion dependencies
pip install -r ingest/requirements.txt

# Install UI dependencies
pip install -r ui/requirements.txt
```

### 3. Environment Configuration
```bash
# Create .env file with your configuration
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
MODEL_NAME=mananthakris/e5-base-ft-abo
DB_PATH=vectordb
PARSE_MODEL=gpt-4o-mini
NLG_MODEL=gpt-4o-mini
EOF

# Load environment variables
export $(cat .env | xargs)
```

### 4. Build Vector Database
```bash
# Option A: Use makefile (recommended)
make seed

# Option B: Direct command
python ingest/rebuild_index.py --parquet data/products_e5-base.parquet --db-path vectordb --collection products --wipe
```

### 5. Start the Application

#### Option A: Start API Server
```bash
# Start the FastAPI server
uvicorn api.main:app --reload --port 8000

# Visit http://localhost:8000/docs for API documentation
```

#### Option B: Start Streamlit UI
```bash
# Start the Streamlit interface
streamlit run ui/app.py

# Open http://localhost:8501 in your browser
```

#### Option C: Use Makefile
```bash
# Rebuild vector database
make seed

# Run tests
make test

# Build Docker images for production
make build
```

## ğŸ³ Production Deployment

This project is **production-ready** with full containerization and automated deployment to Google Cloud Platform (GCP) Cloud Run.

### ğŸ—ï¸ Production Architecture

```mermaid
graph TB
    subgraph "GitHub Actions CI/CD"
        A[Code Push] --> B[Build Docker Images]
        B --> C[Push to GHCR]
        C --> D[Deploy to Cloud Run]
    end
    
    subgraph "GCP Cloud Run"
        D --> E[API Service]
        D --> F[UI Service]
        E --> G[Vector Database]
        F --> E
    end
    
    subgraph "Security & Best Practices"
        H[GitHub Secrets] --> I[Environment Variables]
        J[No Secrets in Code] --> K[Secure Configuration]
        L[Health Checks] --> M[Monitoring]
    end
```

### ğŸš€ Automated Deployment Features

#### âœ… **Containerization**
- **Multi-stage Docker builds** for optimized image sizes
- **Health checks** for both API and UI services
- **CPU-optimized PyTorch** for Cloud Run compatibility
- **Security best practices** (non-root user, minimal base images)

#### âœ… **CI/CD Pipeline**
- **GitHub Actions** automated deployment
- **Container Registry** (GitHub Container Registry)
- **Zero-downtime deployments** with Cloud Run
- **Environment-specific configurations**

#### âœ… **Production Security**
- **Secrets management** via GitHub Secrets (no secrets in code)
- **Environment variables** for configuration
- **Authenticated deployments** with service accounts
- **Network security** with Cloud Run's built-in features

### ğŸ”§ Deployment Configuration

#### Required GitHub Secrets
```bash
# GCP Configuration
GCP_CREDENTIALS          # Service account JSON
GCP_PROJECT_ID          # GCP Project ID
GCP_REGION             # Deployment region

# Application Configuration
OPENAI_API_KEY         # OpenAI API key
OPENAI_BASE_URL        # OpenAI endpoint (optional)
PARSE_MODEL           # LLM model for parsing
NLG_MODEL             # LLM model for generation
```

#### Cloud Run Services
- **API Service**: `shoptalk-api` (1 CPU, 1GB RAM)
- **UI Service**: `shoptalk-ui` (1 CPU, 512MB RAM)
- **Auto-scaling**: 0-100 instances based on traffic
- **Public access**: Unauthenticated (configurable)

### ğŸ› ï¸ Manual Deployment

#### Build and Push Images
```bash
# Build API image
docker build -t ghcr.io/your-org/shoptalk-api:latest ./api

# Build UI image  
docker build -t ghcr.io/your-org/shoptalk-ui:latest ./ui

# Push to registry
docker push ghcr.io/your-org/shoptalk-api:latest
docker push ghcr.io/your-org/shoptalk-ui:latest
```

#### Deploy to Cloud Run
```bash
# Deploy API
gcloud run deploy shoptalk-api \
  --image ghcr.io/your-org/shoptalk-api:latest \
  --region us-central1 \
  --platform managed \
  --allow-unauthenticated \
  --memory 1Gi \
  --cpu 1 \
  --port 8000 \
  --set-env-vars "OPENAI_API_KEY=your-key,DB_PATH=/data"

# Deploy UI
gcloud run deploy shoptalk-ui \
  --image ghcr.io/your-org/shoptalk-ui:latest \
  --region us-central1 \
  --platform managed \
  --allow-unauthenticated \
  --memory 512Mi \
  --cpu 1 \
  --port 8501 \
  --set-env-vars "API_URL=https://your-api-url"
```

### ğŸ“Š Production Monitoring

#### Health Checks
- **API Health**: `GET /health` endpoint
- **UI Health**: Streamlit health endpoint
- **Automatic restarts** on health check failures

#### Logging & Observability
- **Structured logging** with request/response tracking
- **Cloud Run logs** integration
- **Performance metrics** and error tracking
- **Query parsing** and filtering logs

#### Performance Optimization
- **CPU-optimized models** for Cloud Run
- **Efficient vector search** with Chroma
- **Connection pooling** and caching
- **Timeout protection** (30-second limits)

### ğŸ”’ Security Features

#### Secrets Management
- âœ… **No secrets in repository**
- âœ… **GitHub Secrets** for sensitive data
- âœ… **Environment variables** for configuration
- âœ… **Service account authentication**

#### Network Security
- âœ… **HTTPS by default** (Cloud Run)
- âœ… **Authenticated deployments**
- âœ… **Private container registry**
- âœ… **Configurable access controls**

#### Application Security
- âœ… **Input validation** and sanitization
- âœ… **SQL injection protection** (parameterized queries)
- âœ… **Rate limiting** (Cloud Run built-in)
- âœ… **Error handling** without information leakage

### ğŸŒ Production URLs

After deployment, your services will be available at:
- **API**: `https://shoptalk-api-xxx-uc.a.run.app`
- **UI**: `https://shoptalk-ui-xxx-uc.a.run.app`
- **API Docs**: `https://shoptalk-api-xxx-uc.a.run.app/docs`

## ğŸ” Usage Examples

### API Endpoints

#### Search Products
```bash
curl "http://localhost:8000/answer?q=red%20running%20shoes&k=10"
```

#### Health Check
```bash
curl "http://localhost:8000/health"
```

### Natural Language Queries
- "phone case with white flowers"
- "red running shoes under $100"
- "jewelry with diamonds"
- "modern furniture for living room"
- "wireless headphones with noise cancellation"

## ğŸ§ª Testing

```bash
# Run API tests
python -m pytest tests/

# Test specific functionality
python tests/test_api_health.py
python tests/test_embed_and_retreive.py
```

## ğŸ“Š Performance Metrics

The system was evaluated using:
- **Recall@10**: Percentage of relevant items in top 10 results
- **NDCG@10**: Normalized Discounted Cumulative Gain at rank 10
- **Model Comparison**: e5-base outperformed other pre-trained models
- **Fine-tuning**: Triplet loss improved embedding quality for product search

## ğŸ› ï¸ Development

### Project Structure
```
shoptalk-search-assistant/
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py            # Main API server
â”‚   â”œâ”€â”€ llm_helper.py      # LangChain integration
â”‚   â””â”€â”€ requirements.txt   # API dependencies
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ products_e5-base.parquet  # Product embeddings
â”‚   â””â”€â”€ manifest_e5-base-ft-*.json # Model metadata
â”œâ”€â”€ ingest/                # Data ingestion
â”‚   â”œâ”€â”€ ingest.py          # Data loading scripts
â”‚   â”œâ”€â”€ rebuild_index.py   # Vector DB rebuilding
â”‚   â””â”€â”€ requirements.txt   # Ingestion dependencies
â”œâ”€â”€ research/              # Research notebooks
â”‚   â””â”€â”€ eda/               # Exploratory data analysis
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ ui/                    # Streamlit frontend
â”‚   â”œâ”€â”€ app.py            # Streamlit application
â”‚   â””â”€â”€ requirements.txt  # UI dependencies
â”œâ”€â”€ vectordb/             # Chroma vector database
â””â”€â”€ README.md             # This file
```

### Key Features
- **Semantic Search**: Vector-based similarity search
- **Query Understanding**: LLM-powered natural language parsing
- **Smart Filtering**: Category, color, brand, price filtering
- **Result Reranking**: Cross-encoder model for relevance
- **Natural Responses**: LLM-generated conversational answers
- **Error Handling**: Timeout protection and graceful fallbacks
- **Production Ready**: Full containerization with Docker
- **Cloud Native**: Automated deployment to GCP Cloud Run
- **Security First**: No secrets in code, secure configuration
- **CI/CD Pipeline**: Automated testing and deployment

## ğŸ”§ Configuration

### Environment Variables
- `OPENAI_API_KEY`: OpenAI API key for LLM services
- `OPENAI_BASE_URL`: Custom OpenAI endpoint (optional)
- `MODEL_NAME`: Embedding model name
- `DB_PATH`: Vector database path
- `PARSE_MODEL`: LLM model for query parsing
- `NLG_MODEL`: LLM model for response generation

### Model Configuration
- **Embedding Model**: `mananthakris/e5-base-ft-abo` (fine-tuned e5-base)
- **Reranking Model**: `BAAI/bge-reranker-v2-m3`
- **LLM Models**: `gpt-4o-mini` (configurable)

## ğŸ“ˆ Monitoring & Debugging

The system includes comprehensive logging and debugging features:
- Query parsing results
- Filter application logs
- Category distribution analysis
- Performance metrics
- Error tracking

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. Git LFS Issues
```bash
# If you see LFS files as pointers, run:
git lfs install
git lfs pull
```

#### 2. Vector Database Not Found
```bash
# Rebuild the vector database
make seed
```

#### 3. OpenAI API Errors
- Verify your API key is correct in `.env`
- Check your OpenAI account has sufficient credits
- Ensure you have access to the required models

#### 4. Port Already in Use
```bash
# Kill processes on ports 8000 or 8501
lsof -ti:8000 | xargs kill -9
lsof -ti:8501 | xargs kill -9
```

#### 5. Import Errors
```bash
# Ensure you're in the virtual environment
source venv/bin/activate

# Reinstall dependencies
pip install -r api/requirements.txt
pip install -r ui/requirements.txt
pip install -r ingest/requirements.txt
```

#### 6. Search Timeouts
- The system has 30-second timeout protection
- Try simpler queries if complex ones timeout
- Check server logs for specific error messages

### Performance Tips
- Use the fine-tuned model for better results
- Ensure sufficient RAM for vector operations
- Consider using GPU acceleration for large datasets

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

[TBD]

## ğŸ™ Acknowledgments

- ABO Dataset for product data
- Hugging Face for embedding models
- OpenAI for LLM capabilities
- Chroma for vector database
- Streamlit for UI framework